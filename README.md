# Multi-Agent Code Generation Failure Localization and Refinement

This repository contains a system designed to evaluate, localize, and refine failures in multi-agent code generation systems. The framework incorporates multiple agents, each responsible for a specific task in the code generation pipeline. These agents collaborate to generate a solution, and when a failure occurs, the system localizes the source of the failure and provides a minimal set of changes to refine the solution.

## System Overview

The system is built to support the following tasks:

1. **Failure Localization**: Identify which agents are responsible for the failure in the generated code.
2. **Failure Refinement**: Apply minimal changes to the code based on the localized failures to generate a correct solution.

The system operates using a collaborative multi-agent framework where each agent performs a specific task:

- **Product Manager (PM)**: Defines the problem requirements.
- **Architect**: Designs the high-level solution.
- **Engineer**: Implements the solution based on the design.
- **QA Engineer**: Tests the solution for edge cases and correct outputs.

## Files and Functions

### 1. **`failure_localization.py`**

This script implements the failure localization functionality of the system. The goal of this module is to analyze the logs of the agents' actions during the code generation process and localize which agents contributed to the failure.

#### Key Functions:
- **`retrieve_reference_solution(problem_description)`**: Retrieves the reference solution from a local knowledge base to compare against the generated solution.
- **`localize_failure(agent_logs, problem_description)`**: Analyzes the agent logs and compares the actions of each agent against the reference solution to determine the responsibility of each agent for the failure.

### 2. **`refinement.py`**

The `refinement.py` script handles the failure refinement task. Based on the failure localization results, this script generates minimal changes required to fix the generated code. It interacts with OpenAI to provide counterfactual reasoning for the required refinements.

#### Key Functions:
- **`refine_code(problem_description, failed_code, failure_localization, agent_logs, reference_solution)`**: The main function to generate and apply minimal refinements to the failed code. It uses failure localization information to focus on agents' contributions and applies OpenAI's recommendations.
- **`create_refinement_prompt(agent_name, role, action, message, failed_code, reference_solution)`**: Constructs a detailed prompt to guide OpenAI's GPT model to suggest refinements based on the agent’s actions.
- **`reason_with_openai_for_refinement(prompt)`**: Sends the constructed prompt to OpenAI and retrieves the refinement suggestions.
- **`apply_refinement_suggestions(failed_code, refinement_suggestions)`**: Applies the refinements to the failed code based on OpenAI's suggestions.
- **`validate_refined_code(refined_code, problem_description, reference_solution)`**: Validates the refined code by comparing it against test cases and the reference solution.

### 3. **Data Directory Structure**

This repository includes test cases for evaluating the system's performance. The data directory is structured as follows:

#### `test_dataset/`

Each folder inside the `test_dataset` directory corresponds to a specific problem instance and contains the following files:

- **`folder/`**: This folder contains the code generated by the MetaGPT system for a particular problem.
- **`clean_log/`**: This file contains the agent interaction logs, detailing the actions and decisions made by each agent during the code generation process. Each log entry typically includes the agent's name, role, action taken, and the associated message.
- **`label.xlsx/`**: This Excel file contains the manually annotated failure localization results. The labels in this file indicate the responsibility of each agent for the failure, including the assigned score for each agent. The file includes columns such as:
  - `Agent`: The name of the agent responsible (e.g., Alice, Bob, etc.).
  - `Role`: The role of the agent (e.g., Product Manager, Architect, Engineer, QA Engineer).
  - `Failure Responsibility`: A numeric score (between 0 and 1) representing the degree to which the agent is responsible for the failure.
  - `Output Summary`: the summary of the output of each agent.

## Example Workflow

1. **Failure Localization**: 
    - The `failure_localization.py` script analyzes the agent logs and compares the generated code with a reference solution to identify which agents are responsible for the failure. 
    - It assigns a responsibility score to each agent based on their actions and the deviation from the reference solution.

2. **Failure Refinement**: 
    - The `refinement_recommendation.py` script uses the failure localization results to focus on the agents whose actions contributed most to the failure. 
    - It generates refinement suggestions using OpenAI's GPT model and applies the minimal changes to the code.
    - The refined code is then validated against the original problem description and reference solution.

## Usage


2. **Run the System**:
    You can run the failure localization and refinement tasks by executing the following scripts:

    ```bash
    python failure_localization.py
    python refinement.py
    ```

3. **Input Data**:
    For each problem in the dataset, the system expects the following inputs:
    - **Agent Logs**: A list of dictionaries containing logs of the agents’ actions.
    - **Failed Code**: The erroneous code generated by the system.
    - **Reference Solution**: A reference solution for the problem, used for comparison.
    - **Failure Localization Data**: An Excel file containing the manually annotated agent responsibility scores for the failure.

4. **Output**:
    The output will include:
    - **Failure Localization**: A dictionary mapping each agent to their responsibility score for the failure.
    - **Refined Code**: The modified code that fixes the issues identified during failure localization.

## Data Source

The problems used in this repository come from the **Codeforces** competitive programming platform. For each problem, a reference solution is provided, and the code is evaluated against it to identify failures. The agent logs and failure localization annotations are derived from a human-LLM interaction framework.


## Acknowledgements

This work is built upon the **MetaGPT** framework, which is used to generate the code and collect the agent logs. We would like to thank the authors of MetaGPT for their contributions to this work.

